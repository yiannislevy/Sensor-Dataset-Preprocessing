{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3706186c452ebfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:47.654845Z",
     "start_time": "2024-10-28T13:15:47.623941Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()  # Automatically applies Seaborn’s default style\n",
    "from scipy import stats, signal\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604270df0584572",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:48.175714Z",
     "start_time": "2024-10-28T13:15:48.172571Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensor_dtype = np.dtype([\n",
    "    (\"x\", \">f4\"),\n",
    "    (\"y\", \">f4\"),\n",
    "    (\"z\", \">f4\"),\n",
    "    (\"time\", \">i8\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f421acc18e79b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:48.528787Z",
     "start_time": "2024-10-28T13:15:48.523972Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_raw_sensor_data(path):\n",
    "    \"\"\"Load and process sensor data from binary files\"\"\"\n",
    "    acc_files = sorted([f for f in os.listdir(path) if f.endswith('.bin') and 'accelerometer' in f])\n",
    "    gyro_files = sorted([f for f in os.listdir(path) if f.endswith('.bin') and 'gyroscope' in f])\n",
    "    \n",
    "    all_acc_data = []\n",
    "    all_gyro_data = []\n",
    "    \n",
    "    # Load accelerometer data\n",
    "    for file in acc_files:\n",
    "        boot_time_nanos = int(file.split(\"_\")[0]) * 1e6\n",
    "        file_path = os.path.join(path, file)\n",
    "        acc_data = np.fromfile(file_path, dtype=sensor_dtype)\n",
    "        first_event_time = acc_data['time'][0]\n",
    "        corrected_timestamps = ((acc_data['time'] - first_event_time) + boot_time_nanos) / 1e9\n",
    "        corrected_datetimes = pd.to_datetime(corrected_timestamps, unit='s')\n",
    "        df = pd.DataFrame(acc_data[[\"x\", \"y\", \"z\"]].byteswap().newbyteorder())\n",
    "        df['time'] = corrected_datetimes\n",
    "        all_acc_data.append(df)\n",
    "    \n",
    "    # Load gyroscope data\n",
    "    for file in gyro_files:\n",
    "        boot_time_nanos = int(file.split(\"_\")[0]) * 1e6\n",
    "        file_path = os.path.join(path, file)\n",
    "        gyro_data = np.fromfile(file_path, dtype=sensor_dtype)\n",
    "        first_event_time = gyro_data['time'][0]\n",
    "        corrected_timestamps = ((gyro_data['time'] - first_event_time) + boot_time_nanos) / 1e9\n",
    "        corrected_datetimes = pd.to_datetime(corrected_timestamps, unit='s')\n",
    "        df = pd.DataFrame(gyro_data[[\"x\", \"y\", \"z\"]].byteswap().newbyteorder())\n",
    "        df['time'] = corrected_datetimes\n",
    "        all_gyro_data.append(df)\n",
    "        \n",
    "    return pd.concat(all_acc_data), pd.concat(all_gyro_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd69423139708f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:48.761867Z",
     "start_time": "2024-10-28T13:15:48.759123Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sync_sensors(acc_data, gyro_data):\n",
    "    \"\"\"Synchronize accelerometer and gyroscope data\"\"\"\n",
    "    common_start_time = max(acc_data['time'].min(), gyro_data['time'].min())\n",
    "    common_end_time = min(acc_data['time'].max(), gyro_data['time'].max())\n",
    "    \n",
    "    acc_synced = acc_data[(acc_data['time'] >= common_start_time) & (acc_data['time'] <= common_end_time)]\n",
    "    gyro_synced = gyro_data[(gyro_data['time'] >= common_start_time) & (gyro_data['time'] <= common_end_time)]\n",
    "    \n",
    "    return acc_synced.reset_index(drop=True), gyro_synced.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca45ae22909639a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:48.941241Z",
     "start_time": "2024-10-28T13:15:48.938946Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_sampling_rate(data):\n",
    "    \"\"\"Calculate actual sampling rate statistics from timestamps\"\"\"\n",
    "    time_diffs = np.diff(data['time'].astype(np.int64)) / 1e9  # Convert to seconds\n",
    "    sampling_stats = {\n",
    "        'mean_rate': 1 / np.mean(time_diffs),\n",
    "        'std_rate': np.std(1 / time_diffs),\n",
    "        'min_rate': 1 / np.max(time_diffs),\n",
    "        'max_rate': 1 / np.min(time_diffs)\n",
    "    }\n",
    "    return sampling_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6090c11ac4561d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:49.074770Z",
     "start_time": "2024-10-28T13:15:49.072145Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_signal_stats(data):\n",
    "    \"\"\"Calculate comprehensive signal statistics\"\"\"\n",
    "    return {\n",
    "        'range': {\n",
    "            'min': data[['x', 'y', 'z']].min().to_dict(),\n",
    "            'max': data[['x', 'y', 'z']].max().to_dict()\n",
    "        },\n",
    "        'mean': data[['x', 'y', 'z']].mean().to_dict(),\n",
    "        'std': data[['x', 'y', 'z']].std().to_dict(),\n",
    "        'skewness': data[['x', 'y', 'z']].apply(stats.skew).to_dict(),\n",
    "        'kurtosis': data[['x', 'y', 'z']].apply(stats.kurtosis).to_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d1d421294a6ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:49.823180Z",
     "start_time": "2024-10-28T13:15:49.821264Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def analyze_spectrum(data, fs):\n",
    "#     \"\"\"Perform spectral analysis using Welch's method\"\"\"\n",
    "#     freqs, Pxx = signal.welch(data[['x', 'y', 'z']].values, fs=fs, nperseg=1024)\n",
    "#     \n",
    "#     # Calculate power in different frequency bands\n",
    "#     low_mask = (freqs >= 0.1) & (freqs < 3)\n",
    "#     mid_mask = (freqs >= 3) & (freqs < 10)\n",
    "#     high_mask = freqs >= 10\n",
    "#     \n",
    "#     power_dist = {\n",
    "#         'low_freq': {\n",
    "#             'x': np.sum(Pxx[low_mask, 0]),\n",
    "#             'y': np.sum(Pxx[low_mask, 1]),\n",
    "#             'z': np.sum(Pxx[low_mask, 2])\n",
    "#         },\n",
    "#         'mid_freq': {\n",
    "#             'x': np.sum(Pxx[mid_mask, 0]),\n",
    "#             'y': np.sum(Pxx[mid_mask, 1]),\n",
    "#             'z': np.sum(Pxx[mid_mask, 2])\n",
    "#         },\n",
    "#         'high_freq': {\n",
    "#             'x': np.sum(Pxx[high_mask, 0]),\n",
    "#             'y': np.sum(Pxx[high_mask, 1]),\n",
    "#             'z': np.sum(Pxx[high_mask, 2])\n",
    "#         }\n",
    "#     }\n",
    "#     \n",
    "#     return freqs, Pxx, power_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6dbeb4754e9d3c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:50.457615Z",
     "start_time": "2024-10-28T13:15:50.455725Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_path = \"../data/raw\"\n",
    "output_path = \"data_specs\"\n",
    "Path(output_path).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1137d6b2de82ea79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:50.823978Z",
     "start_time": "2024-10-28T13:15:50.822053Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_sessions = []\n",
    "all_acc_data = []\n",
    "all_gyro_data = []\n",
    "session_durations = []\n",
    "sampling_rates_acc = []\n",
    "sampling_rates_gyro = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aec74935f5e75b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:51.912353Z",
     "start_time": "2024-10-28T13:15:51.554968Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 17_meal_2...\n",
      "Processing 1_meal_1...\n",
      "Processing 22_meal_1...\n",
      "Processing 20...\n",
      "Processing 18...\n",
      "Processing 9...\n",
      "Processing 11...\n",
      "Processing 7...\n",
      "Processing 16...\n",
      "Processing 6...\n",
      "Processing 10...\n",
      "Processing 19...\n",
      "Processing 8...\n",
      "Processing 21...\n",
      "Processing 17_meal_1...\n",
      "Processing 1_meal_2...\n",
      "Processing 22_meal_2...\n",
      "Processing 3**...\n",
      "Processing 23...\n",
      "Processing 4...\n",
      "Processing 15...\n",
      "Processing 12...\n",
      "Processing 2...\n",
      "Processing 13...\n",
      "Processing 5...\n",
      "Processing 14...\n"
     ]
    }
   ],
   "source": [
    "for session_dir in Path(base_path).glob(\"*\"):\n",
    "    if not session_dir.is_dir():\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing {session_dir.name}...\")\n",
    "    \n",
    "    # Extract subject info\n",
    "    parts = session_dir.name.split('_')\n",
    "    subject_id = int(re.findall(r'\\d+', parts[0])[0])\n",
    "    meal_num = int(parts[-1]) if len(parts) > 2 else 1\n",
    "    \n",
    "    # Load and process data\n",
    "    acc_data, gyro_data = load_raw_sensor_data(session_dir)\n",
    "    acc_synced, gyro_data = sync_sensors(acc_data, gyro_data)\n",
    "    \n",
    "    # Calculate session duration in minutes\n",
    "    duration = (acc_synced['time'].max() - acc_synced['time'].min()).total_seconds() / 60\n",
    "    session_durations.append(duration)\n",
    "    \n",
    "    # Calculate sampling rates\n",
    "    sampling_rates_acc.append(calculate_sampling_rate(acc_synced))\n",
    "    sampling_rates_gyro.append(calculate_sampling_rate(gyro_data))\n",
    "    \n",
    "    # Store data for overall analysis\n",
    "    all_acc_data.append(acc_synced)\n",
    "    all_gyro_data.append(gyro_data)\n",
    "    all_sessions.append({\n",
    "        'subject_id': subject_id,\n",
    "        'meal_num': meal_num,\n",
    "        'duration': duration\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121facc6b3a71cec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:15:54.182494Z",
     "start_time": "2024-10-28T13:15:52.917518Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Summary:\n",
      "Total Sessions: 26\n",
      "Unique Subjects: 23\n",
      "Total Duration: 576.52 minutes\n",
      "Mean Session Duration: 22.17 minutes\n",
      "\n",
      "Sampling Rates:\n",
      "Accelerometer: 51.84 ± 265.82 Hz\n",
      "Gyroscope: 51.03 ± 105.95 Hz\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Combine all data\n",
    "combined_acc = pd.concat(all_acc_data, ignore_index=True)\n",
    "combined_gyro = pd.concat(all_gyro_data, ignore_index=True)\n",
    "\n",
    "# Calculate mean sampling rate for spectral analysis\n",
    "mean_fs_acc = np.mean([s['mean_rate'] for s in sampling_rates_acc])\n",
    "mean_fs_gyro = np.mean([s['mean_rate'] for s in sampling_rates_gyro])\n",
    "\n",
    "# Generate comprehensive results\n",
    "results = {\n",
    "    'general': {\n",
    "        'total_sessions': len(all_sessions),\n",
    "        'unique_subjects': len(set(s['subject_id'] for s in all_sessions)),\n",
    "        'total_duration_minutes': sum(session_durations),\n",
    "        'mean_duration_minutes': np.mean(session_durations),\n",
    "        'std_duration_minutes': np.std(session_durations)\n",
    "    },\n",
    "    'sampling_rates': {\n",
    "        'accelerometer': {\n",
    "            'mean': np.mean([s['mean_rate'] for s in sampling_rates_acc]),\n",
    "            'std': np.std([s['std_rate'] for s in sampling_rates_acc]),  # Fixed from np.mean to np.std\n",
    "            'min': np.min([s['min_rate'] for s in sampling_rates_acc]),\n",
    "            'max': np.max([s['max_rate'] for s in sampling_rates_acc])\n",
    "        },\n",
    "        'gyroscope': {\n",
    "            'mean': np.mean([s['mean_rate'] for s in sampling_rates_gyro]),\n",
    "            'std': np.std([s['std_rate'] for s in sampling_rates_gyro]),  # Fixed from np.mean to np.std\n",
    "            'min': np.min([s['min_rate'] for s in sampling_rates_gyro]),\n",
    "            'max': np.max([s['max_rate'] for s in sampling_rates_gyro])\n",
    "        }\n",
    "    },\n",
    "    'accelerometer_stats': calculate_signal_stats(combined_acc),\n",
    "    'gyroscope_stats': calculate_signal_stats(combined_gyro)\n",
    "}\n",
    "\n",
    "# Commenting out Spectral Analysis for simplicity, fix later if needed\n",
    "# _, _, acc_spectrum = analyze_spectrum(combined_acc, mean_fs_acc)\n",
    "# _, _, gyro_spectrum = analyze_spectrum(combined_gyro, mean_fs_gyro)\n",
    "# results['spectral_analysis'] = {\n",
    "#     'accelerometer': acc_spectrum,\n",
    "#     'gyroscope': gyro_spectrum\n",
    "# }\n",
    "\n",
    "# Generate visualizations\n",
    "plt.style.use('ggplot')  # Using a safe, guaranteed-to-exist style\n",
    "\n",
    "# 1. Session Duration Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(session_durations, bins=20)\n",
    "plt.title('Session Duration Distribution')\n",
    "plt.xlabel('Duration (minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(os.path.join(output_path, 'session_durations.png'))\n",
    "plt.close()\n",
    "\n",
    "# 2. Sampling Rate Stability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist([s['mean_rate'] for s in sampling_rates_acc], bins=20)\n",
    "plt.title('Accelerometer Sampling Rate Distribution')\n",
    "plt.xlabel('Sampling Rate (Hz)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist([s['mean_rate'] for s in sampling_rates_gyro], bins=20)\n",
    "plt.title('Gyroscope Sampling Rate Distribution')\n",
    "plt.xlabel('Sampling Rate (Hz)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'sampling_rates.png'))\n",
    "plt.close()\n",
    "\n",
    "# Commented out Spectral Power Distribution visualization for simplicity\n",
    "# Uncomment and fix as needed later\n",
    "\n",
    "# Save results to JSON\n",
    "with open(os.path.join(output_path, 'analysis_results.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(f\"Total Sessions: {results['general']['total_sessions']}\")\n",
    "print(f\"Unique Subjects: {results['general']['unique_subjects']}\")\n",
    "print(f\"Total Duration: {results['general']['total_duration_minutes']:.2f} minutes\")\n",
    "print(f\"Mean Session Duration: {results['general']['mean_duration_minutes']:.2f} minutes\")\n",
    "print(\"\\nSampling Rates:\")\n",
    "print(f\"Accelerometer: {results['sampling_rates']['accelerometer']['mean']:.2f} ± {results['sampling_rates']['accelerometer']['std']:.2f} Hz\")\n",
    "print(f\"Gyroscope: {results['sampling_rates']['gyroscope']['mean']:.2f} ± {results['sampling_rates']['gyroscope']['std']:.2f} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79556c8f32d193de",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.style.available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37dca538ff21dca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')  # Choose any that’s reliably listed in `plt.style.available`\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9dd215956d9bd8b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35fecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming data is loaded as 'acc_data' and 'gyro_data'\n",
    "# Load your datasets\n",
    "acc_data = pd.read_csv('accelerometer_data.csv')  # Replace with your actual file path\n",
    "gyro_data = pd.read_csv('gyroscope_data.csv')     # Replace with your actual file path\n",
    "\n",
    "# Parameters (assumed sampling rate)\n",
    "sampling_rate = 100  # Hz, adjust as needed\n",
    "\n",
    "class SensorAnalysis:\n",
    "    def __init__(self, sampling_rate):\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def calculate_sync_gaps(self, acc_data, gyro_data):\n",
    "        \"\"\"Calculate temporal offsets between synchronized sensors\"\"\"\n",
    "        acc_times = acc_data['time'].astype(np.int64)\n",
    "        gyro_times = gyro_data['time'].astype(np.int64)\n",
    "\n",
    "        offset_stats = {\n",
    "            'mean_offset_ms': (acc_times - gyro_times).mean() / 1e6,\n",
    "            'std_offset_ms': (acc_times - gyro_times).std() / 1e6,\n",
    "            'max_offset_ms': (acc_times - gyro_times).max() / 1e6\n",
    "        }\n",
    "        return offset_stats\n",
    "\n",
    "    def analyze_sampling_consistency(self, data):\n",
    "        \"\"\"Analyze sampling interval consistency\"\"\"\n",
    "        intervals = np.diff(data['time'].astype(np.int64)) / 1e9\n",
    "        interval_stats = {\n",
    "            'expected_interval': 1/self.sampling_rate,\n",
    "            'mean_interval': np.mean(intervals),\n",
    "            'std_interval': np.std(intervals),\n",
    "            'confidence_interval': stats.t.interval(0.95, len(intervals)-1,\n",
    "                                                  loc=np.mean(intervals),\n",
    "                                                  scale=stats.sem(intervals))\n",
    "        }\n",
    "        return interval_stats\n",
    "\n",
    "    def analyze_data_completeness(self, data, expected_samples):\n",
    "        \"\"\"Analyze data completeness and continuity\"\"\"\n",
    "        actual_samples = len(data)\n",
    "        missing_ratio = 1 - (actual_samples / expected_samples)\n",
    "\n",
    "        # Analyze timestamp continuity\n",
    "        time_gaps = np.diff(data['time'].astype(np.int64)) / 1e9\n",
    "        continuity_stats = {\n",
    "            'completeness_ratio': 1 - missing_ratio,\n",
    "            'total_gaps': np.sum(time_gaps > (2/self.sampling_rate)),\n",
    "            'max_gap_duration': np.max(time_gaps),\n",
    "            'mean_gap_duration': np.mean(time_gaps[time_gaps > (2/self.sampling_rate)])\n",
    "        }\n",
    "        return continuity_stats\n",
    "\n",
    "    def validate_sensor_ranges(self, data):\n",
    "        \"\"\"Validate sensor measurements against manufacturer specs\"\"\"\n",
    "        # Add your sensor's specifications here\n",
    "        ACC_RANGE = 16  # g, adjust based on your sensor\n",
    "        GYRO_RANGE = 2000  # deg/s, adjust based on your sensor\n",
    "\n",
    "        validation_stats = {\n",
    "            'out_of_range_samples': len(data[\n",
    "                (data['x'].abs() > ACC_RANGE) |\n",
    "                (data['y'].abs() > ACC_RANGE) |\n",
    "                (data['z'].abs() > ACC_RANGE)\n",
    "            ]),\n",
    "            'range_violation_ratio': len(data[\n",
    "                (data['x'].abs() > ACC_RANGE) |\n",
    "                (data['y'].abs() > ACC_RANGE) |\n",
    "                (data['z'].abs() > ACC_RANGE)\n",
    "            ]) / len(data)\n",
    "        }\n",
    "        return validation_stats\n",
    "\n",
    "# Initialize the analysis class\n",
    "analysis = SensorAnalysis(sampling_rate)\n",
    "\n",
    "# Perform synchronization gap analysis\n",
    "sync_gaps = analysis.calculate_sync_gaps(acc_data, gyro_data)\n",
    "print(\"Synchronization Gap Analysis:\")\n",
    "print(sync_gaps)\n",
    "\n",
    "# Analyze sampling consistency for accelerometer data\n",
    "sampling_consistency_acc = analysis.analyze_sampling_consistency(acc_data)\n",
    "print(\"\\nSampling Consistency Analysis (Accelerometer):\")\n",
    "print(sampling_consistency_acc)\n",
    "\n",
    "# Analyze sampling consistency for gyroscope data\n",
    "sampling_consistency_gyro = analysis.analyze_sampling_consistency(gyro_data)\n",
    "print(\"\\nSampling Consistency Analysis (Gyroscope):\")\n",
    "print(sampling_consistency_gyro)\n",
    "\n",
    "# Expected number of samples for data completeness analysis\n",
    "expected_samples = int((acc_data['time'].iloc[-1] - acc_data['time'].iloc[0]) / (1e9 / sampling_rate))\n",
    "\n",
    "# Analyze data completeness for accelerometer data\n",
    "completeness_acc = analysis.analyze_data_completeness(acc_data, expected_samples)\n",
    "print(\"\\nData Completeness Analysis (Accelerometer):\")\n",
    "print(completeness_acc)\n",
    "\n",
    "# Analyze data completeness for gyroscope data\n",
    "completeness_gyro = analysis.analyze_data_completeness(gyro_data, expected_samples)\n",
    "print(\"\\nData Completeness Analysis (Gyroscope):\")\n",
    "print(completeness_gyro)\n",
    "\n",
    "# Validate sensor ranges for accelerometer data\n",
    "validation_acc = analysis.validate_sensor_ranges(acc_data)\n",
    "print(\"\\nSensor Range Validation (Accelerometer):\")\n",
    "print(validation_acc)\n",
    "\n",
    "# Validate sensor ranges for gyroscope data\n",
    "validation_gyro = analysis.validate_sensor_ranges(gyro_data)\n",
    "print(\"\\nSensor Range Validation (Gyroscope):\")\n",
    "print(validation_gyro)\n",
    "\n",
    "# Consolidated Analysis Summary\n",
    "print(\"\\n--- Consolidated Analysis Summary ---\")\n",
    "print(\"Synchronization Gap Analysis:\", sync_gaps)\n",
    "print(\"Sampling Consistency (Accelerometer):\", sampling_consistency_acc)\n",
    "print(\"Sampling Consistency (Gyroscope):\", sampling_consistency_gyro)\n",
    "print(\"Data Completeness (Accelerometer):\", completeness_acc)\n",
    "print(\"Data Completeness (Gyroscope):\", completeness_gyro)\n",
    "print(\"Sensor Range Validation (Accelerometer):\", validation_acc)\n",
    "print(\"Sensor Range Validation (Gyroscope):\", validation_gyro)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
